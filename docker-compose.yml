services:
  # ─────────────── MINIO ───────────────
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"      # S3 API
      - "9001:9001"      # Web console
    environment:
      MINIO_ROOT_USER:     minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - ./mnt/minio:/data

  # ───────────── SPARK MASTER ─────────────
  spark-master:
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - INIT_DAEMON_STEP=setup_spark
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - HADOOP_CONF_DIR=/spark/conf
    volumes:
      - ./workspace:/opt/data          # mã nguồn / notebook / script
      - ./mnt/spark-conf:/spark/conf       # spark-defaults.conf có spark.jars.packages

  # ──────────── SPARK WORKER 1 ────────────
  spark-worker-1:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-1
    depends_on: [spark-master]
    ports:
      - "8081:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=8g
      - SPARK_DAEMON_MEMORY=1g
      - HADOOP_CONF_DIR=/spark/conf
    volumes:
      - ./workspace:/opt/data
      - ./mnt/spark-conf:/spark/conf

  # ──────────── SPARK WORKER 2 ────────────
  spark-worker-2:
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-2
    depends_on: [spark-master]
    ports:
      - "8082:8081"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - SPARK_WORKER_CORES=4
      - SPARK_WORKER_MEMORY=8g
      - SPARK_DAEMON_MEMORY=1g
      - HADOOP_CONF_DIR=/spark/conf
    volumes:
      - ./workspace:/opt/data
      - ./mnt/spark-conf:/spark/conf

  # ────────── SPARK HISTORY SERVER ─────────
  spark-history-server:
    image: bde2020/spark-history-server:3.3.0-hadoop3.3
    container_name: spark-history-server
    depends_on: [spark-master]
    ports:
      - "18081:18081"
    volumes:
      - /tmp/spark-events-local:/tmp/spark-events   # giữ nguyên sự kiện

  # ─────────── JUPYTER NOTEBOOK ────────────
  jupyter-notebook:
    image: jupyter/pyspark-notebook:spark-3.3.0
    container_name: jupyter-notebook
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
      - spark-history-server
    ports:
      - "8888:8888"
      - "4040:4040"      # Spark UI của job trong notebook
    environment:
      - PYSPARK_DRIVER_PYTHON=jupyter
      - PYSPARK_DRIVER_PYTHON_OPTS="notebook --no-browser --ip=0.0.0.0 --port=8888 --NotebookApp.token=''"
      - SPARK_MASTER=spark://spark-master:7077
    command: >
      start-notebook.sh
        --NotebookApp.token=''
        --NotebookApp.password=''
    volumes:
      - ./workspace:/opt/data            # notebook lưu vào workspace
      - ./mnt/spark-conf:/spark/conf


    # ─────────── Hive Metastore (PostgreSQL) ───────────
  postgres-metastore:
    image: postgres:13
    container_name: hive-metastore-db
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
    volumes:
      - ./mnt/postgres-metastore-data:/var/lib/postgresql/data
      - ./mnt/airflow/init-airflow-db.sql:/docker-entrypoint-initdb.d/init-airflow-db.sql:ro

  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    environment:
      - SERVICE_NAME=metastore
      - DB_DRIVER=postgres
      - HIVE_METASTORE_DB_TYPE=postgres
      - HIVE_METASTORE_DB_HOST=postgres-metastore
      - HIVE_METASTORE_DB_PORT=5432
      - HIVE_METASTORE_USER=hive
      - HIVE_METASTORE_PASS=hive
      - HIVE_METASTORE_DB=metastore
    depends_on:
      - postgres-metastore
    ports:
      - "9083:9083"
    volumes:
      - ./mnt/hive-conf/postgresql-42.7.3.jar:/opt/hive/lib/postgres.jar:ro 
      - ./mnt/hive-conf/hive-site.xml:/opt/hive/conf/hive-site.xml:ro
      - ./mnt/hive-conf/hadoop-aws-3.3.6.jar:/opt/hive/lib/hadoop-aws-3.3.6.jar:ro
      - ./mnt/hive-conf/aws-java-sdk-bundle-1.12.619.jar:/opt/hive/lib/aws-java-sdk-bundle-1.12.619.jar:ro
  # ─────────── Apache Airflow ───────────
  airflow:
    build: ./docker/airflow
    container_name: airflow
    restart: always
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres-metastore:5432/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AIRFLOW_CONN_S3_DEFAULT=s3://minioadmin:minioadmin@minio:9000
      - AIRFLOW_CONN_SPARK_DEFAULT=spark://spark://spark-master:7077
      - AIRFLOW_CONN_HIVE_METASTORE=thrift://hive-metastore:9083
    volumes:
      - ./mnt/airflow/dags:/opt/airflow/dags
      - ./mnt/airflow/logs:/opt/airflow/logs
      - ./mnt/airflow/plugins:/opt/airflow/plugins
      - ./mnt/great_expectations:/opt/airflow/great_expectations
      - ./mnt/spark-conf:/opt/spark/conf
      - ./mnt/airflow/scripts:/opt/scripts
    ports:
      - "8083:8080"
    depends_on:
      - hive-metastore
      - postgres-metastore
    command: >
      bash -c "
        airflow db upgrade &&
        airflow users create \
          --username admin \
          --firstname Admin \
          --lastname User \
          --role Admin \
          --email admin@example.com \
          --password admin  &&
        exec airflow scheduler & exec airflow webserver
      "
  # ─────────── Trino ───────────
  trino:
    image: trinodb/trino:latest
    container_name: trino
    restart: always
    depends_on:
      - hive-metastore
    ports:
      - "8084:8080"
    volumes:
      - ./mnt/trino/etc/config.properties:/etc/trino/config.properties
      - ./mnt/trino/etc/jvm.config:/etc/trino/jvm.config
      - ./mnt/trino/etc/catalog:/etc/trino/catalog

  superset:
    build: ./docker/superset
    container_name: superset
    restart: always
    environment:
      - SUPERSET_LOAD_EXAMPLES=no
      - SUPERSET_SECRET_KEY=tIUahn1k73Kl9MlTLk0o50asJfva1267cPt+xN5S4HSzYedS11M5FKxd
    volumes:
      - ./mnt/superset:/app/superset_home
    ports:
      - "8088:8088"
    depends_on:
      - trino
    command: >
      bash -c "superset db upgrade &&
               superset fab create-admin --username admin --firstname Superset --lastname Admin --email admin@example.com --password admin ||
               superset fab reset-password --username admin --password admin &&
               superset init &&
               exec superset run -h 0.0.0.0 -p 8088 --with-threads --reload --debugger"